{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working from gpuserver0\n",
    "#venv used\n",
    "#ts_env                   /home/shuhangli/miniconda3/envs/ts_env\n",
    "from prometheus_api_client import PrometheusConnect, MetricsList, Metric, MetricRangeDataFrame\n",
    "from prometheus_api_client.utils import parse_datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pprint\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "import argparse\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_contour,\n",
    "    plot_param_importances,\n",
    ")\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler \n",
    "\n",
    "from darts.datasets import ElectricityDataset\n",
    "from darts.models import TCNModel, LinearRegressionModel, NBEATSModel, TiDEModel, TSMixerModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape, mape, rmse, mae \n",
    "from darts.utils.likelihood_models import GaussianLikelihood, QuantileRegression\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import (\n",
    "    Scaler,\n",
    "    MissingValuesFiller,\n",
    "    Mapper,\n",
    "    InvertibleMapper,\n",
    ")\n",
    "from darts.metrics import mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/shuhangli/tstest/GEM1h.csv', parse_dates=True, index_col='Time')\n",
    "#remove  µA from the data and convert to float\n",
    "for column in df.columns:\n",
    "    # Check if the column data type is object, indicating it might contain strings\n",
    "    if df[column].dtype == 'object':\n",
    "        # Remove 'µA' and convert to float\n",
    "        df[column] = df[column].str.replace('µA', '').str.strip().astype(float)\n",
    "\n",
    "data_min = df.max().max()\n",
    "data_max = df.min().min()\n",
    "print(data_min, data_max)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_HV = pd.read_csv('/home/shuhangli/tstest/GEMHV1h.csv', parse_dates=True, index_col='Time')\n",
    "df_HV.columns = df_HV.columns.str.replace('u', 'u_HV_')\n",
    "for column in df_HV.columns:\n",
    "    if df_HV[column].dtype == 'object':\n",
    "        df_HV[column] = df_HV[column].str.replace('kV', '').str.strip().astype(float)\n",
    "df_HV.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CM = pd.read_csv('/home/shuhangli/tstest/CM_VI1h.csv', parse_dates=True, index_col='Time')\n",
    "df_CM['Voltage'] = df_CM['Voltage'].str.replace('kV', '').str.strip().astype(float)\n",
    "df_CM['Current'] = df_CM['Current'].str.replace('µA', '').str.strip().astype(float)\n",
    "df_CM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CAD = pd.read_csv('/home/shuhangli/tstest/CAD1h.csv', parse_dates=True, index_col='Time')\n",
    "\n",
    "def convert_units(column):\n",
    "    if df_CAD[column].dtype == 'object':\n",
    "        # Remove the unit and convert to float\n",
    "        df_CAD[column] = df_CAD[column].str.replace('kHz', '').str.replace('Hz', '').str.strip().astype(float)\n",
    "        # Convert kHz to Hz\n",
    "        if 'kHz' in column:\n",
    "            df_CAD[column] = df_CAD[column] * 1000\n",
    "    return df_CAD[column]\n",
    "\n",
    "# Process each column\n",
    "for column in df_CAD.columns:\n",
    "    df_CAD[column] = convert_units(column)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_CAD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint\n",
    "df_cov = df_HV.join(df_CM).join(df_CAD)\n",
    "series_cov_raw = TimeSeries.from_dataframe(df_cov)\n",
    "covfiller = MissingValuesFiller()\n",
    "series_cov = covfiller.transform(series_cov_raw, method='quadratic')\n",
    "Scaler_cov = Scaler(MinMaxScaler())\n",
    "series_cov_scaled = Scaler_cov.fit_transform(series_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_raw = TimeSeries.from_dataframe(df)\n",
    "\n",
    "series_list_raw = []\n",
    "for col in df.columns:\n",
    "    ts = TimeSeries.from_dataframe(df, value_cols=col)\n",
    "    series_list_raw.append(ts)\n",
    "\n",
    "filler = MissingValuesFiller()\n",
    "series = filler.transform(series_raw, method=\"quadratic\")\n",
    "Scaler_data = Scaler(MinMaxScaler())\n",
    "series_scaled = Scaler_data.fit_transform(series)\n",
    "#global_scaler = Mapper(lambda x: 1- (x-data_min)/(data_max - data_min) )\n",
    "#series_scaled = global_scaler.transform(series)\n",
    "#series_scaled.plot()\n",
    "\n",
    "\n",
    "series_list = filler.transform(series_list_raw, method=\"quadratic\")\n",
    "Scaler_tslist = Scaler(scaler=MinMaxScaler(),global_fit=True, verbose=False)\n",
    "#series_list_scaled = Scaler_tslist.fit_transform(series_list)\n",
    "# sepereate the data into training and validation\n",
    "length = len(series_list[0])\n",
    "val_len = int(length/2)\n",
    "\n",
    "series_list_scaled = Scaler_tslist.fit_transform(series_list)\n",
    "\n",
    "train_series = [ts[: -(1 * val_len)] for ts in series_list_scaled]\n",
    "val_series = [ts[-(1 * val_len): ] for ts in series_list_scaled]\n",
    "test_series = [ts[-val_len:] for ts in series_list_scaled]\n",
    "\n",
    "series_cov_scaled_list = [series_cov_scaled for i in range(len(series_list))]\n",
    "\n",
    "\n",
    "\n",
    "#combine train_series to a multi-variate time series\n",
    "train = series_scaled[: -(1 * val_len)]\n",
    "val = series_scaled[-(1 * val_len): ]\n",
    "test = series_scaled[-val_len:]\n",
    "\n",
    "#test_scaler = Scaler()\n",
    "#train_series[0] = test_scaler.fit_transform(train_series[0])\n",
    "#val_series[0] = test_scaler.transform(val_series[0])\n",
    "#test_series[0] = test_scaler.transform(test_series[0])\n",
    "series_scaled.plot()\n",
    "#print(train_series[0])\n",
    "#train_series[0].plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some fixed parameters that will be the same for all models\n",
    "BATCH_SIZE = 16\n",
    "MAX_N_EPOCHS = 300\n",
    "NR_EPOCHS_VAL_PERIOD = 1\n",
    "MAX_SAMPLES_PER_TS = 1000\n",
    "\n",
    "def build_fit_tcn_model(\n",
    "    in_len,\n",
    "    out_len,\n",
    "    kernel_size,\n",
    "    num_filters,\n",
    "    weight_norm,\n",
    "    dilation_base,\n",
    "    dropout,\n",
    "    lr,\n",
    "    likelihood=None,\n",
    "    callbacks=None,\n",
    "):\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    # throughout training we'll monitor the validation loss for early stopping\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.00, patience=10, verbose=True)\n",
    "    if callbacks is None:\n",
    "        callbacks = [early_stopper]\n",
    "    else:\n",
    "        callbacks = [early_stopper] + callbacks\n",
    "\n",
    "    # detect if a GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        pl_trainer_kwargs = {\n",
    "            \"accelerator\": \"gpu\",\n",
    "            \"devices\": [0],\n",
    "            #\"gpus\": -1,\n",
    "            #\"auto_select_gpus\": True,\n",
    "            \"callbacks\": callbacks,\n",
    "        }\n",
    "        num_workers = 4\n",
    "    else:\n",
    "        pl_trainer_kwargs = {\"callbacks\": callbacks}\n",
    "        num_workers = 0\n",
    "\n",
    "    encoders = None\n",
    "\n",
    "    # build the TCN model\n",
    "    model = TCNModel(\n",
    "        input_chunk_length=in_len,\n",
    "        output_chunk_length=out_len,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_epochs=MAX_N_EPOCHS,\n",
    "        nr_epochs_val_period=NR_EPOCHS_VAL_PERIOD,\n",
    "        kernel_size=kernel_size,\n",
    "        num_filters=num_filters,\n",
    "        weight_norm=weight_norm,\n",
    "        dilation_base=dilation_base,\n",
    "        dropout=dropout,\n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        add_encoders=encoders,\n",
    "        likelihood=likelihood,\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "        model_name=\"tcn_model\",\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit( \n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n",
    "        past_covariates=series_cov_scaled,\n",
    "        val_past_covariates=series_cov_scaled,\n",
    "        #num_loader_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # reload best model over course of training\n",
    "    model = TCNModel.load_from_checkpoint(\"tcn_model\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_fit_nbeats_model(\n",
    "    in_len,\n",
    "    out_len,\n",
    "    generic_architecture,\n",
    "    num_stacks,\n",
    "    num_blocks,\n",
    "    num_layers,\n",
    "    layer_widths,\n",
    "    dropout,\n",
    "    lr,\n",
    "    likelihood=None,\n",
    "    callbacks=None,\n",
    "):\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    # throughout training we'll monitor the validation loss for early stopping\n",
    "    early_stopper = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=True)\n",
    "    if callbacks is None:\n",
    "        callbacks = [early_stopper]\n",
    "    else:\n",
    "        callbacks = [early_stopper] + callbacks\n",
    "\n",
    "    # detect if a GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        pl_trainer_kwargs = {\n",
    "            \"accelerator\": \"gpu\",\n",
    "            \"devices\": [0],\n",
    "            \"callbacks\": callbacks,\n",
    "        }\n",
    "        num_workers = 4\n",
    "    else:\n",
    "        pl_trainer_kwargs = {\"callbacks\": callbacks}\n",
    "        num_workers = 0\n",
    "\n",
    "    # build the NBEATS model\n",
    "    model = NBEATSModel(\n",
    "        input_chunk_length=in_len,\n",
    "        output_chunk_length=out_len,\n",
    "        generic_architecture=generic_architecture,\n",
    "        num_stacks=num_stacks,\n",
    "        num_blocks=num_blocks,\n",
    "        num_layers=num_layers,\n",
    "        layer_widths=layer_widths,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_epochs=MAX_N_EPOCHS,\n",
    "        nr_epochs_val_period=NR_EPOCHS_VAL_PERIOD,\n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        likelihood=likelihood,\n",
    "        dropout=dropout,\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "        model_name=\"nbeats_model\",\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # train the model\n",
    "    model.fit( \n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n",
    "        past_covariates=series_cov_scaled,\n",
    "        val_past_covariates=series_cov_scaled,\n",
    "    )\n",
    "\n",
    "    # reload best model over course of training\n",
    "    model = NBEATSModel.load_from_checkpoint(\"nbeats_model\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_fit_tide_model(\n",
    "    in_len,\n",
    "    out_len,\n",
    "    output_chunk_shift,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    decoder_output_dim,\n",
    "    hidden_size,\n",
    "    temporal_width_past,\n",
    "    temporal_width_future,\n",
    "    temporal_decoder_hidden,\n",
    "    use_layer_norm,\n",
    "    dropout,\n",
    "    lr,\n",
    "    likelihood=None,\n",
    "    callbacks=None,\n",
    "):\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    \n",
    "\n",
    "    # throughout training we'll monitor the validation loss for early stopping\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.00, patience=10, verbose=True)\n",
    "    if callbacks is None:\n",
    "        callbacks = [early_stopper]\n",
    "    else:\n",
    "        callbacks = [early_stopper] + callbacks\n",
    "\n",
    "    # detect if a GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        pl_trainer_kwargs = {\n",
    "            \"accelerator\": \"gpu\",\n",
    "            \"devices\": [0],\n",
    "            \"callbacks\": callbacks,\n",
    "        }\n",
    "        num_workers = 4\n",
    "    else:\n",
    "        pl_trainer_kwargs = {\"callbacks\": callbacks}\n",
    "        num_workers = 0\n",
    "\n",
    "    # build the TiDE model\n",
    "    model = TiDEModel(\n",
    "        input_chunk_length=in_len,\n",
    "        output_chunk_length=out_len,\n",
    "        output_chunk_shift=output_chunk_shift,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        decoder_output_dim=decoder_output_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        temporal_width_past=temporal_width_past,\n",
    "        temporal_width_future=temporal_width_future,\n",
    "        temporal_decoder_hidden=temporal_decoder_hidden,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        dropout=dropout,\n",
    "        use_static_covariates=False,  \n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        likelihood=likelihood,\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "        model_name=\"tide_model\",\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "        n_epochs = MAX_N_EPOCHS,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n",
    "        past_covariates=series_cov_scaled,\n",
    "        val_past_covariates=series_cov_scaled,\n",
    "    )\n",
    "\n",
    "    # Reload best model over course of training\n",
    "    model = TiDEModel.load_from_checkpoint(\"tide_model\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_fit_tsmixer_model(\n",
    "    in_len,\n",
    "    out_len,\n",
    "    output_chunk_shift,\n",
    "    hidden_size,\n",
    "    ff_size,\n",
    "    num_blocks,\n",
    "    activation,\n",
    "    dropout,\n",
    "    norm_type,\n",
    "    normalize_before,\n",
    "    lr,\n",
    "    likelihood=None,\n",
    "    callbacks=None,\n",
    "):\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    # early stopping\n",
    "    early_stopper = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=True)\n",
    "    if callbacks is None:\n",
    "        callbacks = [early_stopper]\n",
    "    else:\n",
    "        callbacks.append(early_stopper)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        pl_trainer_kwargs = {\n",
    "            \"accelerator\": \"gpu\",\n",
    "            \"devices\": [0],\n",
    "            \"callbacks\": callbacks,\n",
    "        }\n",
    "        num_workers = 4\n",
    "    else:\n",
    "        pl_trainer_kwargs = {\"callbacks\": callbacks}\n",
    "        num_workers = 0\n",
    "\n",
    "    # build the TSMixer model\n",
    "    model = TSMixerModel(\n",
    "        input_chunk_length=in_len,\n",
    "        output_chunk_length=out_len,\n",
    "        output_chunk_shift=output_chunk_shift,\n",
    "        hidden_size=hidden_size,\n",
    "        ff_size=ff_size,\n",
    "        num_blocks=num_blocks,\n",
    "        activation=activation,\n",
    "        dropout=dropout,\n",
    "        norm_type=norm_type,\n",
    "        normalize_before=normalize_before,\n",
    "        use_static_covariates=False,\n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        likelihood=likelihood,\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "        model_name=\"tsmixer_model\",\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "        n_epochs=MAX_N_EPOCHS,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n",
    "        past_covariates=series_cov_scaled,\n",
    "        val_past_covariates=series_cov_scaled,\n",
    "    )\n",
    "\n",
    "    # Reload best model over course of training\n",
    "    model = TSMixerModel.load_from_checkpoint(\"tsmixer_model\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def eval_model(preds, name, train_set=train, val_set=val):\n",
    "    smapes = smape(preds, val_set)\n",
    "    print(\"{} sMAPE: {:.2f} +- {:.2f}\".format(name, np.mean(smapes), np.std(smapes)))\n",
    "    #mae and mse\n",
    "    maes = mae(preds, val_set)\n",
    "    print(\"{} MAE: {:.4f} +- {:.4f}\".format(name, np.mean(maes), np.std(maes)))\n",
    "    rmses = rmse(preds, val_set)\n",
    "    print(\"{} RMSE: {:.4f} +- {:.4f}\".format(name, np.mean(rmses), np.std(rmses)))\n",
    "\n",
    "def eval_model_series(preds, name, train_set=train, val_set=val):\n",
    "    smapes = smape(preds, val_set)\n",
    "    print(\"{} sMAPE: {:.2f} +- {:.2f}\".format(name, np.mean(smapes), np.std(smapes)))\n",
    "    #mae and mse\n",
    "    maes = mae(preds, val_set)\n",
    "    print(\"{} MAE: {:.4f} +- {:.4f}\".format(name, np.mean(maes), np.std(maes)))\n",
    "    rmses = rmse(preds, val_set)\n",
    "    print(\"{} RMSE: {:.4f} +- {:.4f}\".format(name, np.mean(rmses), np.std(rmses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegressionModel(lags=60, \n",
    "                                 lags_past_covariates=10,\n",
    "                                 output_chunk_length=3,\n",
    "                                 )\n",
    "lr_model.fit(train, \n",
    "             past_covariates=series_cov_scaled\n",
    "             )\n",
    "lr_preds = lr_model.predict(series=val[:60], n=60, \n",
    "                            past_covariates=series_cov_scaled,\n",
    "                            num_samples=1)\n",
    "historical_forecasts_lr = lr_model.historical_forecasts(series=val, retrain=False, past_covariates=series_cov_scaled, forecast_horizon=1)\n",
    "eval_model(lr_preds, \"LR model forecasts\")\n",
    "\n",
    "eval_model(historical_forecasts_lr, \"LR model historical forecasts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_fit_tcn_model(\n",
    "    in_len=60,\n",
    "    out_len=40,\n",
    "    kernel_size=5,\n",
    "    num_filters=5,\n",
    "    weight_norm=False,\n",
    "    dilation_base=3,\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    #likelihood=QuantileRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 13\n",
    "preds = model.predict(series=train, n=120, num_samples=100)\n",
    "eval_model(preds, \"TCN\", train_set=train, val_set=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loop over all the series\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    Scaler_data.inverse_transform(train).univariate_component(i).plot()\n",
    "    Scaler_data.inverse_transform(val).univariate_component(i).plot(label=\"actual\")\n",
    "    Scaler_data.inverse_transform(preds).univariate_component(i).plot(label=\"forecast\")\n",
    "    Scaler_data.inverse_transform(lr_preds).univariate_component(i).plot(label=\"LR forecast\")\n",
    "#eval_model(preds, \"First TCN model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nbeats = build_fit_nbeats_model(\n",
    "    in_len=60,\n",
    "    out_len=30,\n",
    "    generic_architecture=True,\n",
    "    num_stacks=10,\n",
    "    num_blocks=1,\n",
    "    num_layers=4,\n",
    "    layer_widths=256,\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    likelihood=QuantileRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_nbeats.predict(series=train, n=120, \n",
    "                             num_samples=1000\n",
    "                             )\n",
    "eval_model(preds, \"NBEATS\", train_set=train, val_set=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#val['u303'].plot()\n",
    "#preds['u303'].plot()\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    Scaler_data.inverse_transform(train).univariate_component(i).plot()\n",
    "    Scaler_data.inverse_transform(val).univariate_component(i).plot(label=\"actual\")\n",
    "    Scaler_data.inverse_transform(preds).univariate_component(i).plot(label=\"forecast\")\n",
    "    Scaler_data.inverse_transform(lr_preds).univariate_component(i).plot(label=\"LR forecast\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(lr_preds.n_components):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    train.univariate_component(i).plot()\n",
    "    val.univariate_component(i).plot(label=\"actual\")\n",
    "    lr_preds.univariate_component(i).plot(label=\"forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_len = 60\n",
    "out_len = 40\n",
    "output_chunk_shift = 0\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "decoder_output_dim = 16\n",
    "hidden_size = 128\n",
    "temporal_width_past = 8\n",
    "temporal_width_future = 8\n",
    "temporal_decoder_hidden = 32\n",
    "use_layer_norm = True\n",
    "dropout = 0.1\n",
    "lr = 1e-3\n",
    "\n",
    "# Build and fit the model\n",
    "model_tide = build_fit_tide_model(\n",
    "    in_len=in_len,\n",
    "    out_len=out_len,\n",
    "    output_chunk_shift=output_chunk_shift,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    decoder_output_dim=decoder_output_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    temporal_width_past=temporal_width_past,\n",
    "    temporal_width_future=temporal_width_future,\n",
    "    temporal_decoder_hidden=temporal_decoder_hidden,\n",
    "    use_layer_norm=use_layer_norm,\n",
    "    dropout=dropout,\n",
    "    lr=lr,\n",
    "    #likelihood=QuantileRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tide = model_tide.predict(series=val[:60], n=60, num_samples=1, past_covariates=series_cov_scaled)\n",
    "hist_forecast_tide = model_tide.historical_forecasts(series=val, retrain = False, num_samples=1, past_covariates=series_cov_scaled, forecast_horizon=1)\n",
    "eval_model(pred_tide, \"TiDE forecast\", train_set=train, val_set=val)\n",
    "eval_model(hist_forecast_tide, \"TiDE historical forecast\", train_set=train, val_set=val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    Scaler_data.inverse_transform(train).univariate_component(i).plot()\n",
    "    #Scaler_data.inverse_transform(lr_preds).univariate_component(i).plot(label=\"LR forecast\")\n",
    "    Scaler_data.inverse_transform(historical_forecasts_lr).univariate_component(i).plot(label=\"LR historical forecast\")\n",
    "    #Scaler_data.inverse_transform(pred_tide).univariate_component(i).plot(label=\"TiDE forecast\")\n",
    "    Scaler_data.inverse_transform(hist_forecast_tide).univariate_component(i).plot(label=\"tide historical forecast\")\n",
    "    Scaler_data.inverse_transform(val).univariate_component(i).plot(label=\"actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_len = 60\n",
    "out_len = 2\n",
    "output_chunk_shift = 0\n",
    "hidden_size = 256\n",
    "ff_size = 256\n",
    "num_blocks = 8\n",
    "activation = 'ReLU'\n",
    "dropout = 0.08\n",
    "norm_type = 'TimeBatchNorm2d'\n",
    "normalize_before = True\n",
    "lr = 1e-3\n",
    "\n",
    "# Build and fit the model\n",
    "model_tsmixer = build_fit_tsmixer_model(\n",
    "    in_len=in_len,\n",
    "    out_len=out_len,\n",
    "    output_chunk_shift=output_chunk_shift,\n",
    "    hidden_size=hidden_size,\n",
    "    ff_size=ff_size,\n",
    "    num_blocks=num_blocks,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    norm_type=norm_type,\n",
    "    normalize_before=normalize_before,\n",
    "    lr=lr,\n",
    "    #likelihood=QuantileRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tmixer = model_tsmixer.predict(series=train, n=val_len, \n",
    "                                    num_samples=1)\n",
    "eval_model(pred_tmixer, \"TSMixer\", train_set=train, val_set=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    Scaler_data.inverse_transform(train).univariate_component(i).plot()\n",
    "    Scaler_data.inverse_transform(val).univariate_component(i).plot(label=\"actual\")\n",
    "    Scaler_data.inverse_transform(pred_tmixer).univariate_component(i).plot(label=\"forecast\")\n",
    "    Scaler_data.inverse_transform(lr_preds).univariate_component(i).plot(label=\"LR forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter optimization, this does not work...\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torchmetrics import MeanAbsoluteError, MeanAbsolutePercentageError, MetricCollection\n",
    "\n",
    "def train_model_tsmix(model_args, callbacks):\n",
    "    torch_metrics = MetricCollection([MeanAbsolutePercentageError(), MeanAbsoluteError()])\n",
    "    # Create the model using model_args from Ray Tune\n",
    "    model = TSMixerModel(\n",
    "        input_chunk_length=60,\n",
    "        output_chunk_length=30,\n",
    "        n_epochs=100,\n",
    "        torch_metrics=torch_metrics,\n",
    "        pl_trainer_kwargs={\"callbacks\": callbacks, \"enable_progress_bar\": True},\n",
    "        **model_args)\n",
    "\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        past_covariates=series_cov_scaled,\n",
    "        val_past_covariates=series_cov_scaled,\n",
    "    )\n",
    "\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_MeanAbsolutePercentageError\",\n",
    "    patience=5,\n",
    "    min_delta=0.00,\n",
    "    mode='min',\n",
    ")\n",
    "# set up ray tune callback\n",
    "tune_callback = TuneReportCheckpointCallback(\n",
    "    {\n",
    "        \"loss\": \"val_loss\",\n",
    "        \"MAPE\": \"val_MeanAbsolutePercentageError\",\n",
    "    },\n",
    "    on=\"validation_end\",\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"hidden_size\": tune.choice([64, 128]),\n",
    "    \"ff_size\": tune.choice([64, 128]),\n",
    "    \"num_blocks\": tune.choice([2, 4, 6]),\n",
    "    \"dropout\": tune.uniform(0, 0.3),\n",
    "    \"normalize_before\": tune.choice([True, False]),\n",
    "}\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=list(config.keys()),\n",
    "    metric_columns=[\"loss\", \"MAPE\", \"training_iteration\"],\n",
    ")\n",
    "\n",
    "resources_per_trial = {\"cpu\": 1, \"gpu\": 2}\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=1000, grace_period=3, reduction_factor=2)\n",
    "\n",
    "train_fn_with_parameters = tune.with_parameters(\n",
    "    train_model_tsmix, callbacks=[my_stopper, tune_callback],\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_fn_with_parameters,\n",
    "    resources_per_trial=resources_per_trial,\n",
    "    # Using a metric instead of loss allows for\n",
    "    # comparison between different likelihood or loss functions.\n",
    "    metric=\"MAPE\",  # any value in TuneReportCallback.\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    name=\"tune_darts\",\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomaly detection\n",
    "from darts.ad import (\n",
    "    ForecastingAnomalyModel,\n",
    "    KMeansScorer,\n",
    "    NormScorer,\n",
    "    WassersteinScorer,\n",
    "\n",
    ")\n",
    "from darts.ad.utils import (\n",
    "    eval_metric_from_binary_prediction,\n",
    "    eval_metric_from_scores,\n",
    "    show_anomalies_from_scores,\n",
    ")\n",
    "anomaly_model_lr = ForecastingAnomalyModel(\n",
    "    model=lr_model,\n",
    "    scorer=[\n",
    "        NormScorer(ord=1, component_wise=True),\n",
    "        WassersteinScorer(window=2, window_agg=True, component_wise=True),\n",
    "    ],\n",
    ")\n",
    "\n",
    "START = 0.1\n",
    "anomaly_model_lr.fit(train, start=START, allow_model_training=False, verbose=True)\n",
    "\n",
    "anomaly_scores, model_forecasting = anomaly_model_lr.score(val, start=START, verbose=True, return_model_prediction=True, forecast_horizon=3)\n",
    "pred_start = model_forecasting.start_time()\n",
    "\n",
    "model_forecasting.univariate_component(0).plot()\n",
    "val.univariate_component(0).plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [1, 10]\n",
    "scorer_names = [f\"{scorer}_{w}\" for scorer, w in zip(anomaly_model_lr.scorers, windows)]\n",
    "show_anomalies_from_scores(\n",
    "    series=val,\n",
    "    pred_scores=anomaly_scores,\n",
    "    pred_series=model_forecasting,\n",
    "    window=windows,\n",
    "    title=\"Anomaly results using a forecasting method\",\n",
    "    names_of_scorers=scorer_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
